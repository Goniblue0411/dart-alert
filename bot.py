#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, json, re, io, zipfile, html
import requests
from datetime import datetime, timedelta

# =========================
# Secrets (GitHub repo Secrets)
# =========================
DART_API_KEY = os.environ["DART_API_KEY"].strip()
TG_BOT_TOKEN = os.environ["TG_BOT_TOKEN"].strip()
TG_CHAT_ID   = os.environ["TG_CHAT_ID"].strip()

# =========================
# Config (workflow env override ê°€ëŠ¥)
# =========================
LOOKBACK_DAYS = int(os.getenv("LOOKBACK_DAYS", "3"))

# ì‹œì¥ í•„í„° (ë¹ˆ ê°’ì´ë©´ ì „ì²´)
# Y=KOSPI, K=KOSDAQ, N=KONEX, E=OTHER
MARKET_CLASSES = [x.strip().upper() for x in os.getenv("MARKET_CLASSES", "Y,K,N").split(",") if x.strip()]

# í˜ì´ì§€ë„¤ì´ì…˜
MAX_PAGES  = int(os.getenv("MAX_PAGES", "12"))
PAGE_COUNT = int(os.getenv("PAGE_COUNT", "100"))

STATE_PATH = "state.json"
SEEN_MAX   = 8000

# =========================
# Filters (ì •ì±…)
# =========================
# 1) ë³´ê³ ì„œëª…: ìœ ìƒ/ë¬´ìƒ/ìœ ë¬´ìƒ "ê²°ì •"ë§Œ ëŒ€ìƒìœ¼ë¡œ
INC_REPORT = re.compile(
    r"(ìœ ìƒì¦ì\s*ê²°ì •|ìœ ìƒì¦ìê²°ì •|ë¬´ìƒì¦ì\s*ê²°ì •|ë¬´ìƒì¦ìê²°ì •|ìœ ë¬´ìƒì¦ì\s*ê²°ì •|ìœ ë¬´ìƒì¦ìê²°ì •)",
    re.I
)

# 2) ì›ë¬¸ì—ì„œ ì œ3ìë°°ì •ì€ ë¬´ì¡°ê±´ ì œì™¸
EXC_3RD = re.compile(r"(ì œ\s*3\s*ì\s*ë°°ì •|ì œ3ìë°°ì •)", re.I)

# 3) ì›ë¬¸ì—ì„œ í—ˆìš©ë˜ëŠ” ë°°ì • ë°©ì‹
ALLOW_GENERAL = re.compile(r"(ì¼ë°˜\s*ì£¼ì£¼\s*ë°°ì •|ì¼ë°˜ì£¼ì£¼ë°°ì •)", re.I)
ALLOW_SHAREHOLDER = re.compile(r"(ì£¼ì£¼\s*ë°°ì •|ì£¼ì£¼ë°°ì •|êµ¬ì£¼ì£¼\s*ì²­ì•½|êµ¬ì£¼ì£¼ì²­ì•½|êµ¬ì£¼ì£¼)", re.I)

# =========================
# URLs
# =========================
LIST_URL = "https://opendart.fss.or.kr/api/list.json"
DOC_URL  = "https://opendart.fss.or.kr/api/document.xml"
VIEW_URL = "https://dart.fss.or.kr/dsaf001/main.do?rcpNo={}"
TG_SEND  = "https://api.telegram.org/bot{}/sendMessage"

S = requests.Session()
S.headers.update({"User-Agent": "dart-alert-github-actions/3.0"})

TG_MAX = 4096

# =========================
# state.json
# =========================
def load_state():
    try:
        with open(STATE_PATH, "r", encoding="utf-8") as f:
            st = json.load(f)
        if not isinstance(st, dict):
            return {"seen": []}
        if "seen" not in st or not isinstance(st["seen"], list):
            st["seen"] = []
        return st
    except Exception:
        return {"seen": []}

def save_state(st):
    seen = st.get("seen", [])
    if not isinstance(seen, list):
        seen = []
    st["seen"] = seen[-SEEN_MAX:]
    with open(STATE_PATH, "w", encoding="utf-8") as f:
        json.dump(st, f, ensure_ascii=False, indent=2)

def is_seen(st, rcept_no: str) -> bool:
    return rcept_no in set(st.get("seen", []))

def mark_seen(st, rcept_no: str):
    st.setdefault("seen", []).append(rcept_no)

# =========================
# Telegram
# =========================
def tg_send(text: str, button_url: str | None = None):
    payload = {
        "chat_id": TG_CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": True,
    }
    if button_url:
        payload["reply_markup"] = json.dumps(
            {"inline_keyboard": [[{"text": "ğŸ“„ DART ì—´ê¸°", "url": button_url}]]},
            ensure_ascii=False
        )
    r = S.post(TG_SEND.format(TG_BOT_TOKEN), data=payload, timeout=30)
    r.raise_for_status()

def tg_send_safe(text: str, button_url: str | None = None):
    if len(text) <= TG_MAX:
        tg_send(text, button_url)
        return
    tg_send(text[: TG_MAX - 40] + "\n\n(â€¦ì¤‘ëµ)", button_url)

# =========================
# DART list.json (pagination)
# =========================
def market_ok(corp_cls: str) -> bool:
    corp_cls = (corp_cls or "").strip().upper()
    if not MARKET_CLASSES:
        return True
    return corp_cls in MARKET_CLASSES

def fetch_list_pages():
    end_de = datetime.now().strftime("%Y%m%d")
    bgn_de = (datetime.now() - timedelta(days=LOOKBACK_DAYS)).strftime("%Y%m%d")

    out = []
    for p in range(1, MAX_PAGES + 1):
        j = S.get(
            LIST_URL,
            params=dict(
                crtfc_key=DART_API_KEY,
                bgn_de=bgn_de,
                end_de=end_de,
                pblntf_ty="B",
                page_no=p,
                page_count=PAGE_COUNT,
                sort="date",
                sort_mth="desc",
            ),
            timeout=30,
        ).json()

        status = j.get("status")
        if status == "013":
            break
        if status != "000":
            raise RuntimeError(f"LIST error {status}: {j.get('message')}")

        lst = j.get("list") or []
        if not lst:
            break
        out.extend(lst)

        # total_count ê¸°ë°˜ ì¢…ë£Œ(ìˆì„ ë•Œë§Œ)
        try:
            total = int(j.get("total_count") or 0)
            pc    = int(j.get("page_count") or PAGE_COUNT)
            if total and total <= p * pc:
                break
        except Exception:
            pass

    # rcept_no ì¤‘ë³µ ì œê±°
    seen = set()
    dedup = []
    for it in out:
        rno = (it.get("rcept_no") or "").strip()
        if not rno or rno in seen:
            continue
        seen.add(rno)
        dedup.append(it)

    # ì˜¤ë˜ëœ ê²ƒë¶€í„° ì²˜ë¦¬
    dedup.sort(key=lambda x: (x.get("rcept_dt", ""), x.get("rcept_no", "")))
    return dedup

# =========================
# document.xml fetch + textify
# =========================
def _xml_to_text(xml_bytes: bytes) -> str:
    s = xml_bytes.decode("utf-8", errors="ignore")
    s = re.sub(r"(?i)<br\s*/?>", "\n", s)
    s = re.sub(r"(?i)</(tr|p|div|li|h\d)>", "\n", s)
    s = re.sub(r"<[^>]+>", " ", s)
    s = html.unescape(s)
    s = re.sub(r"[ \t\r\f\v]+", " ", s)
    s = re.sub(r"\n\s+", "\n", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def fetch_document_text(rcept_no: str) -> str:
    r = S.get(DOC_URL, params={"crtfc_key": DART_API_KEY, "rcept_no": rcept_no}, timeout=60)
    r.raise_for_status()
    raw = r.content

    texts = []
    is_zip = (r.headers.get("Content-Type", "").lower().find("zip") >= 0) or (raw[:2] == b"PK")
    if is_zip:
        with zipfile.ZipFile(io.BytesIO(raw)) as z:
            for name in z.namelist():
                if name.lower().endswith((".xml", ".html", ".htm")):
                    try:
                        texts.append(_xml_to_text(z.read(name)))
                    except Exception:
                        pass
    else:
        texts.append(_xml_to_text(raw))

    return "\n\n".join([t for t in texts if t])

# =========================
# classify allocation + type
# =========================
def classify_event_type(report_nm: str) -> str:
    rn = report_nm or ""
    if re.search(r"ë¬´ìƒì¦ì", rn):
        return "ë¬´ìƒ"
    if re.search(r"ìœ ë¬´ìƒì¦ì", rn):
        return "ìœ ë¬´ìƒ"
    if re.search(r"ìœ ìƒì¦ì", rn):
        return "ìœ ìƒ"
    return "N/A"

def classify_allocation(doc_text: str) -> str:
    if ALLOW_GENERAL.search(doc_text):
        return "ì¼ë°˜ì£¼ì£¼ë°°ì •"
    if ALLOW_SHAREHOLDER.search(doc_text):
        return "ì£¼ì£¼ë°°ì •"
    return "N/A"

# =========================
# Field extraction helpers (ìš´ì˜í˜•)
# =========================
def _norm_ws(s: str) -> str:
    return re.sub(r"\s{2,}", " ", (s or "")).strip()

def pick_first_by_labels(text: str, labels: list[str], maxlen: int = 120) -> str:
    """
    textì—ì„œ labels ì¤‘ í•˜ë‚˜ê°€ í¬í•¨ëœ ë¼ì¸ì˜ 'ê°’'ì„ 1ê°œ ë½‘ìŒ.
    í‘œê°€ ì¤„ë¡œ ë¶„ë¦¬ë˜ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë‹¤ìŒ ì¤„ 1ê°œê¹Œì§€ ë³´ì¡°ë¡œ ë¶™ì„.
    """
    if not text:
        return "N/A"
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    for i, ln in enumerate(lines):
        for lb in labels:
            if lb in ln:
                after = ln.split(lb, 1)[1]
                after = re.sub(r"^[\s:\-Â·â€¢\)]+", "", after).strip()
                after = _norm_ws(after)

                if len(after) < 2 and i + 1 < len(lines):
                    nxt = _norm_ws(lines[i + 1])
                    # ë‹¤ìŒ ì¤„ì´ ë˜ ë¼ë²¨ì´ë©´ ë¶™ì´ì§€ ì•ŠìŒ
                    if nxt and not any(x in nxt for x in labels):
                        after = _norm_ws((after + " " + nxt).strip())

                if after:
                    return after[:maxlen].strip()
    return "N/A"

def pick_multi_by_labels(text: str, labels: list[str], max_items: int = 6, maxlen_each: int = 80) -> str:
    """
    ì—¬ëŸ¬ ë¼ë²¨(ìš°ë¦¬ì‚¬ì£¼/êµ¬ì£¼ì£¼/ì¼ë°˜ê³µëª¨ ì²­ì•½ì¼ ë“±)ì„ ëª¨ì•„ì„œ í•œ ì¤„ë¡œ í•©ì¹¨.
    """
    if not text:
        return "N/A"
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    hits = []
    for i, ln in enumerate(lines):
        for lb in labels:
            if lb in ln:
                after = ln.split(lb, 1)[1]
                after = re.sub(r"^[\s:\-Â·â€¢\)]+", "", after).strip()
                after = _norm_ws(after)
                if len(after) < 2 and i + 1 < len(lines):
                    nxt = _norm_ws(lines[i + 1])
                    if nxt and lb not in nxt:
                        after = _norm_ws((after + " " + nxt).strip())
                if after:
                    hits.append(f"{lb}: {after[:maxlen_each].strip()}")

    # ì¤‘ë³µ ì œê±°(ìˆœì„œ ìœ ì§€)
    uniq, seen = [], set()
    for h in hits:
        key = re.sub(r"\s+", " ", h)
        if key in seen:
            continue
        seen.add(key)
        uniq.append(h)
        if len(uniq) >= max_items:
            break

    if not uniq:
        return "N/A"
    out = " / ".join(uniq)
    return out[:350] + ("â€¦" if len(out) > 350 else "")

def extract_money_purpose(text: str) -> str:
    """
    ìê¸ˆì¡°ë‹¬ì˜ëª©ì : í‘œ/ë¬¸ì¥ ì¼€ì´ìŠ¤ê°€ ë‹¤ì–‘í•´ì„œ
    - 'ìê¸ˆì¡°ë‹¬' / 'ìê¸ˆì‚¬ìš©ëª©ì ' ë¼ì¸ì´ ìˆìœ¼ë©´ ê·¸ ë¼ì¸ ê°’
    - ì—†ìœ¼ë©´ ì‹œì„¤/ìš´ì˜/ì±„ë¬´ìƒí™˜/íƒ€ë²•ì¸/ê¸°íƒ€ í‚¤ì›Œë“œê°€ ë¶™ì€ ë¼ì¸ ì¼ë¶€ë¥¼ ìš”ì•½
    """
    # 1) ëŒ€í‘œ ë¼ë²¨ ìš°ì„ 
    v = pick_first_by_labels(text, [
        "ìê¸ˆì¡°ë‹¬ì˜ ëª©ì ", "ìê¸ˆì¡°ë‹¬ ëª©ì ", "ìê¸ˆì¡°ë‹¬ì˜ëª©ì ",
        "ìê¸ˆì˜ ì‚¬ìš©ëª©ì ", "ìê¸ˆì‚¬ìš©ëª©ì ", "ìê¸ˆ ì‚¬ìš© ëª©ì ",
        "ì¡°ë‹¬ìê¸ˆì˜ ì‚¬ìš©ëª©ì ", "ì¡°ë‹¬ ìê¸ˆì˜ ì‚¬ìš©ëª©ì ",
    ], maxlen=160)
    if v != "N/A":
        return v

    # 2) í‘œì—ì„œ ëª©ì  í•­ëª©+ê¸ˆì•¡ì´ í©ì–´ì ¸ ë‚˜ì˜¤ëŠ” ê²½ìš°ë¥¼ ì•½ì‹ìœ¼ë¡œ ìˆ˜ì§‘
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    keys = ["ì‹œì„¤", "ìš´ì˜", "ì±„ë¬´", "íƒ€ë²•ì¸", "ê¸°íƒ€", "ì—°êµ¬", "M&A", "ì¸ìˆ˜", "íˆ¬ì"]
    amt_pat = re.compile(r"(\d{1,3}(?:,\d{3})+|\d+)\s*ì›")
    hits = []
    for ln in lines:
        if any(k in ln for k in keys) and amt_pat.search(ln):
            hits.append(_norm_ws(ln))
        if len(hits) >= 4:
            break
    if hits:
        out = " / ".join(hits)
        return out[:200] + ("â€¦" if len(out) > 200 else "")
    return "N/A"

def extract_fields(doc_text: str) -> dict:
    """
    ìš”ì²­í•œ í•­ëª©:
    - ìê¸ˆì¡°ë‹¬ì˜ëª©ì 
    - ì‹ ì£¼ë°°ì •ê¸°ì¤€ì¼
    - ì˜ˆì •ê°€
    - í™•ì •ì¼
    - ì‹ ì£¼ì¸ìˆ˜ê¶Œìƒì¥ì˜ˆì •ê¸°ê°„
    - ì²­ì•½ì¼
    - ì‹ ì£¼ì˜ìƒì¥ì˜ˆì •ì¼
    """
    fields = {}

    fields["ìê¸ˆì¡°ë‹¬ì˜ëª©ì "] = extract_money_purpose(doc_text)

    fields["ì‹ ì£¼ë°°ì •ê¸°ì¤€ì¼"] = pick_first_by_labels(doc_text, [
        "ì‹ ì£¼ë°°ì •ê¸°ì¤€ì¼", "ì‹ ì£¼ ë°°ì • ê¸°ì¤€ì¼", "ë°°ì •ê¸°ì¤€ì¼", "ë°°ì • ê¸°ì¤€ì¼",
        "ì‹ ì£¼ë°°ì • ê¸°ì¤€ì¼", "ê¶Œë¦¬ë½ ê¸°ì¤€ì¼",
    ])

    # ì˜ˆì •ê°€ / ë°œí–‰ê°€(ì˜ˆì •) ë“± ë¬¸êµ¬ ë³€í˜• ëŒ€ì‘
    fields["ì˜ˆì •ê°€"] = pick_first_by_labels(doc_text, [
        "ì˜ˆì •ë°œí–‰ê°€ì•¡", "ì˜ˆì • ë°œí–‰ê°€ì•¡", "ë°œí–‰ê°€ì•¡(ì˜ˆì •)", "ë°œí–‰ê°€ì•¡ (ì˜ˆì •)",
        "ì˜ˆì •ë°œí–‰ê°€", "ì˜ˆì • ë°œí–‰ê°€", "ì˜ˆì •ê°€", "ì˜ˆì •ê°€ì•¡",
        "1ì£¼ë‹¹ ë°œí–‰ê°€ì•¡(ì˜ˆì •)", "1ì£¼ë‹¹ ë°œí–‰ê°€ì•¡ (ì˜ˆì •)",
    ], maxlen=140)

    fields["í™•ì •ì¼"] = pick_first_by_labels(doc_text, [
        "ë°œí–‰ê°€ì•¡í™•ì •ì¼", "ë°œí–‰ê°€ì•¡ í™•ì •ì¼",
        "í™•ì •ì¼", "ê°€ê²©í™•ì •ì¼", "ê°€ê²© í™•ì •ì¼",
        "ë°œí–‰ê°€ í™•ì •ì¼", "ë°œí–‰ê°€ì•¡ì˜ í™•ì •ì¼",
    ])

    # ì‹ ì£¼ì¸ìˆ˜ê¶Œ ìƒì¥ì˜ˆì •ê¸°ê°„(ê¶Œë¦¬ ìƒì¥ê¸°ê°„)
    fields["ì‹ ì£¼ì¸ìˆ˜ê¶Œìƒì¥ì˜ˆì •ê¸°ê°„"] = pick_first_by_labels(doc_text, [
        "ì‹ ì£¼ì¸ìˆ˜ê¶Œì¦ì„œ ìƒì¥ì˜ˆì •ê¸°ê°„", "ì‹ ì£¼ì¸ìˆ˜ê¶Œì¦ì„œìƒì¥ì˜ˆì •ê¸°ê°„",
        "ì‹ ì£¼ì¸ìˆ˜ê¶Œ ìƒì¥ì˜ˆì •ê¸°ê°„", "ì‹ ì£¼ì¸ìˆ˜ê¶Œìƒì¥ì˜ˆì •ê¸°ê°„",
        "ì‹ ì£¼ì¸ìˆ˜ê¶Œì¦ì„œ ìƒì¥ê¸°ê°„", "ì‹ ì£¼ì¸ìˆ˜ê¶Œì¦ì„œìƒì¥ê¸°ê°„",
        "ì‹ ì£¼ì¸ìˆ˜ê¶Œ ìƒì¥ê¸°ê°„", "ì‹ ì£¼ì¸ìˆ˜ê¶Œìƒì¥ê¸°ê°„",
    ], maxlen=160)

    # ì²­ì•½ì¼(ìš°ë¦¬ì‚¬ì£¼/êµ¬ì£¼ì£¼/ì¼ë°˜ê³µëª¨ ë“± ë‹¤ì¤‘ ë¼ë²¨ í•©ì¹˜ê¸°)
    fields["ì²­ì•½ì¼"] = pick_multi_by_labels(doc_text, [
        "ìš°ë¦¬ì‚¬ì£¼ì¡°í•© ì²­ì•½ì¼", "ìš°ë¦¬ì‚¬ì£¼ì¡°í•©ì²­ì•½ì¼",
        "êµ¬ì£¼ì£¼ ì²­ì•½ì¼", "êµ¬ì£¼ì£¼ì²­ì•½ì¼",
        "ì¼ë°˜ê³µëª¨ ì²­ì•½ì¼", "ì¼ë°˜ê³µëª¨ì²­ì•½ì¼",
        "ì¼ë°˜ì²­ì•½ì¼",
        "ì²­ì•½ì¼",
    ])

    fields["ì‹ ì£¼ì˜ìƒì¥ì˜ˆì •ì¼"] = pick_first_by_labels(doc_text, [
        "ì‹ ì£¼ì˜ ìƒì¥ì˜ˆì •ì¼", "ì‹ ì£¼ì˜ìƒì¥ì˜ˆì •ì¼",
        "ì‹ ì£¼ ìƒì¥ì˜ˆì •ì¼", "ì‹ ì£¼ìƒì¥ì˜ˆì •ì¼",
        "ì‹ ì£¼ê¶Œ ìƒì¥ì˜ˆì •ì¼", "ì‹ ì£¼ê¶Œìƒì¥ì˜ˆì •ì¼",
        "ìƒì¥ì˜ˆì •ì¼",
    ])

    return fields

# =========================
# main
# =========================
def main():
    st = load_state()
    sent = 0

    items = fetch_list_pages()

    for it in items:
        rno = (it.get("rcept_no") or "").strip()
        if not rno:
            continue
        if is_seen(st, rno):
            continue

        corp_cls = (it.get("corp_cls") or "").strip().upper()
        if not market_ok(corp_cls):
            mark_seen(st, rno)
            continue

        rpt_nm = (it.get("report_nm") or "").strip()
        if not INC_REPORT.search(rpt_nm):
            mark_seen(st, rno)
            continue

        # ì›ë¬¸ í…ìŠ¤íŠ¸ ê°€ì ¸ì™€ì„œ ë°°ì •ë°©ì‹/ì œ3ì ì—¬ë¶€/í•„ë“œ ì¶”ì¶œ
        try:
            doc_text = fetch_document_text(rno)
        except Exception:
            mark_seen(st, rno)
            continue

        # ì œ3ìë°°ì • í¬í•¨ì´ë©´ ë¬´ì¡°ê±´ ì œì™¸
        if EXC_3RD.search(doc_text):
            mark_seen(st, rno)
            continue

        alloc = classify_allocation(doc_text)
        if alloc == "N/A":
            mark_seen(st, rno)
            continue

        fields = extract_fields(doc_text)
        ev_type = classify_event_type(rpt_nm)

        corp = (it.get("corp_name") or "N/A").strip()
        rcept_dt = (it.get("rcept_dt") or "").strip()
        url = VIEW_URL.format(rno)
        market_name = {"Y": "KOSPI", "K": "KOSDAQ", "N": "KONEX", "E": "OTHER"}.get(corp_cls, corp_cls or "N/A")

        lines = [
            "ğŸ“Œ <b>ì¦ì ê³µì‹œ ê°ì§€</b>",
            f"â€¢ íšŒì‚¬: <b>{html.escape(corp)}</b> <i>({html.escape(market_name)})</i>",
            f"â€¢ ìœ í˜•: <b>{html.escape(ev_type)}</b> / ë°°ì •: <b>{html.escape(alloc)}</b>",
            f"â€¢ ì ‘ìˆ˜ì¼: {html.escape(rcept_dt or 'N/A')}",
            f"â€¢ ê³µì‹œëª…: {html.escape(rpt_nm)}",
            "",
            f"â€¢ ìê¸ˆì¡°ë‹¬ì˜ëª©ì : {html.escape(fields['ìê¸ˆì¡°ë‹¬ì˜ëª©ì '])}",
            f"â€¢ ì‹ ì£¼ë°°ì •ê¸°ì¤€ì¼: {html.escape(fields['ì‹ ì£¼ë°°ì •ê¸°ì¤€ì¼'])}",
            f"â€¢ ì˜ˆì •ê°€: {html.escape(fields['ì˜ˆì •ê°€'])}",
            f"â€¢ í™•ì •ì¼: {html.escape(fields['í™•ì •ì¼'])}",
            f"â€¢ ì‹ ì£¼ì¸ìˆ˜ê¶Œìƒì¥ì˜ˆì •ê¸°ê°„: {html.escape(fields['ì‹ ì£¼ì¸ìˆ˜ê¶Œìƒì¥ì˜ˆì •ê¸°ê°„'])}",
            f"â€¢ ì²­ì•½ì¼: {html.escape(fields['ì²­ì•½ì¼'])}",
            f"â€¢ ì‹ ì£¼ì˜ìƒì¥ì˜ˆì •ì¼: {html.escape(fields['ì‹ ì£¼ì˜ìƒì¥ì˜ˆì •ì¼'])}",
        ]

        tg_send_safe("\n".join(lines), button_url=url)
        mark_seen(st, rno)
        sent += 1

    save_state(st)
    print(f"OK sent={sent} seen={len(st.get('seen', []))}")

if __name__ == "__main__":
    main()
