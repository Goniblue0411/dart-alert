#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, json, re, io, zipfile, html
import requests
from datetime import datetime, timedelta

# =========================
# Secrets (GitHub repo Secrets)
# =========================
DART_API_KEY = os.environ["DART_API_KEY"].strip()
TG_BOT_TOKEN = os.environ["TG_BOT_TOKEN"].strip()
TG_CHAT_ID   = os.environ["TG_CHAT_ID"].strip()

# =========================
# Config
# =========================
LOOKBACK_DAYS = int(os.getenv("LOOKBACK_DAYS", "3"))

# ì‹œì¥ í•„í„° (ë¹ˆ ê°’ì´ë©´ ì „ì²´)
# Y=KOSPI, K=KOSDAQ, N=KONEX, E=OTHER
MARKET_CLASSES = [x.strip().upper() for x in os.getenv("MARKET_CLASSES", "Y,K,N").split(",") if x.strip()]

# í˜ì´ì§€ë„¤ì´ì…˜
MAX_PAGES  = int(os.getenv("MAX_PAGES", "12"))
PAGE_COUNT = int(os.getenv("PAGE_COUNT", "100"))

STATE_PATH = "state.json"
SEEN_MAX   = 8000

# =========================
# Filters (ì •ì±…)
# =========================
# 1) ë³´ê³ ì„œëª…: ìœ ìƒ/ë¬´ìƒ/ìœ ë¬´ìƒ "ê²°ì •"ë§Œ ëŒ€ìƒìœ¼ë¡œ
INC_REPORT = re.compile(
    r"(ìœ ìƒì¦ì\s*ê²°ì •|ìœ ìƒì¦ìê²°ì •|ë¬´ìƒì¦ì\s*ê²°ì •|ë¬´ìƒì¦ìê²°ì •|ìœ ë¬´ìƒì¦ì\s*ê²°ì •|ìœ ë¬´ìƒì¦ìê²°ì •)",
    re.I
)

# 2) ì›ë¬¸ì—ì„œ ì œ3ìë°°ì •ì€ ë¬´ì¡°ê±´ ì œì™¸
EXC_3RD = re.compile(r"(ì œ\s*3\s*ì\s*ë°°ì •|ì œ3ìë°°ì •)", re.I)

# 3) ì›ë¬¸ì—ì„œ í—ˆìš©ë˜ëŠ” ë°°ì • ë°©ì‹
ALLOW_GENERAL = re.compile(r"(ì¼ë°˜\s*ì£¼ì£¼\s*ë°°ì •|ì¼ë°˜ì£¼ì£¼ë°°ì •)", re.I)
ALLOW_SHAREHOLDER = re.compile(r"(ì£¼ì£¼\s*ë°°ì •|ì£¼ì£¼ë°°ì •|êµ¬ì£¼ì£¼\s*ì²­ì•½|êµ¬ì£¼ì£¼ì²­ì•½|êµ¬ì£¼ì£¼)", re.I)

# =========================
# URLs
# =========================
LIST_URL = "https://opendart.fss.or.kr/api/list.json"
DOC_URL  = "https://opendart.fss.or.kr/api/document.xml"
VIEW_URL = "https://dart.fss.or.kr/dsaf001/main.do?rcpNo={}"
TG_SEND  = "https://api.telegram.org/bot{}/sendMessage"

S = requests.Session()
S.headers.update({"User-Agent": "dart-alert-github-actions/4.0"})

TG_MAX = 4096

# =========================
# state.json
# =========================
def load_state():
    try:
        with open(STATE_PATH, "r", encoding="utf-8") as f:
            st = json.load(f)
        if not isinstance(st, dict):
            return {"seen": []}
        if "seen" not in st or not isinstance(st["seen"], list):
            st["seen"] = []
        return st
    except Exception:
        return {"seen": []}

def save_state(st):
    seen = st.get("seen", [])
    if not isinstance(seen, list):
        seen = []
    st["seen"] = seen[-SEEN_MAX:]
    with open(STATE_PATH, "w", encoding="utf-8") as f:
        json.dump(st, f, ensure_ascii=False, indent=2)

def is_seen(st, rcept_no: str) -> bool:
    return rcept_no in set(st.get("seen", []))

def mark_seen(st, rcept_no: str):
    st.setdefault("seen", []).append(rcept_no)

# =========================
# Telegram
# =========================
def tg_send(text: str, button_url: str | None = None):
    payload = {
        "chat_id": TG_CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": True,
    }
    if button_url:
        payload["reply_markup"] = json.dumps(
            {"inline_keyboard": [[{"text": "ğŸ“„ DART ì—´ê¸°", "url": button_url}]]},
            ensure_ascii=False
        )
    r = S.post(TG_SEND.format(TG_BOT_TOKEN), data=payload, timeout=30)
    r.raise_for_status()

def tg_send_safe(text: str, button_url: str | None = None):
    if len(text) <= TG_MAX:
        tg_send(text, button_url)
        return
    tg_send(text[: TG_MAX - 40] + "\n\n(â€¦ì¤‘ëµ)", button_url)

# =========================
# DART list.json (pagination)
# =========================
def market_ok(corp_cls: str) -> bool:
    corp_cls = (corp_cls or "").strip().upper()
    if not MARKET_CLASSES:
        return True
    return corp_cls in MARKET_CLASSES

def fetch_list_pages():
    end_de = datetime.now().strftime("%Y%m%d")
    bgn_de = (datetime.now() - timedelta(days=LOOKBACK_DAYS)).strftime("%Y%m%d")

    out = []
    for p in range(1, MAX_PAGES + 1):
        j = S.get(
            LIST_URL,
            params=dict(
                crtfc_key=DART_API_KEY,
                bgn_de=bgn_de,
                end_de=end_de,
                pblntf_ty="B",
                page_no=p,
                page_count=PAGE_COUNT,
                sort="date",
                sort_mth="desc",
            ),
            timeout=30,
        ).json()

        status = j.get("status")
        if status == "013":
            break
        if status != "000":
            raise RuntimeError(f"LIST error {status}: {j.get('message')}")

        lst = j.get("list") or []
        if not lst:
            break
        out.extend(lst)

        # total_count ê¸°ë°˜ ì¢…ë£Œ(ìˆì„ ë•Œë§Œ)
        try:
            total = int(j.get("total_count") or 0)
            pc    = int(j.get("page_count") or PAGE_COUNT)
            if total and total <= p * pc:
                break
        except Exception:
            pass

    # rcept_no ì¤‘ë³µ ì œê±°
    seen = set()
    dedup = []
    for it in out:
        rno = (it.get("rcept_no") or "").strip()
        if not rno or rno in seen:
            continue
        seen.add(rno)
        dedup.append(it)

    # ì˜¤ë˜ëœ ê²ƒë¶€í„° ì²˜ë¦¬
    dedup.sort(key=lambda x: (x.get("rcept_dt", ""), x.get("rcept_no", "")))
    return dedup

# =========================
# document.xml fetch + textify
# =========================
def _xml_to_text(xml_bytes: bytes) -> str:
    s = xml_bytes.decode("utf-8", errors="ignore")
    s = re.sub(r"(?i)<br\s*/?>", "\n", s)
    s = re.sub(r"(?i)</(tr|p|div|li|h\d)>", "\n", s)
    s = re.sub(r"<[^>]+>", " ", s)
    s = html.unescape(s)
    s = re.sub(r"[ \t\r\f\v]+", " ", s)
    s = re.sub(r"\n\s+", "\n", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def fetch_document_text(rcept_no: str) -> str:
    r = S.get(DOC_URL, params={"crtfc_key": DART_API_KEY, "rcept_no": rcept_no}, timeout=60)
    r.raise_for_status()
    raw = r.content

    texts = []
    is_zip = (r.headers.get("Content-Type", "").lower().find("zip") >= 0) or (raw[:2] == b"PK")
    if is_zip:
        with zipfile.ZipFile(io.BytesIO(raw)) as z:
            for name in z.namelist():
                if name.lower().endswith((".xml", ".html", ".htm")):
                    try:
                        texts.append(_xml_to_text(z.read(name)))
                    except Exception:
                        pass
    else:
        texts.append(_xml_to_text(raw))

    return "\n\n".join([t for t in texts if t])

# =========================
# classify allocation + type
# =========================
def classify_event_type(report_nm: str) -> str:
    rn = report_nm or ""
    if re.search(r"ë¬´ìƒì¦ì", rn):
        return "ë¬´ìƒ"
    if re.search(r"ìœ ë¬´ìƒì¦ì", rn):
        return "ìœ ë¬´ìƒ"
    if re.search(r"ìœ ìƒì¦ì", rn):
        return "ìœ ìƒ"
    return "N/A"

def classify_allocation(doc_text: str) -> str:
    if ALLOW_GENERAL.search(doc_text):
        return "ì¼ë°˜ì£¼ì£¼ë°°ì •"
    if ALLOW_SHAREHOLDER.search(doc_text):
        return "ì£¼ì£¼ë°°ì •"
    return "N/A"

# =========================
# Field extraction
# =========================
def _norm_ws(s: str) -> str:
    return re.sub(r"\s{2,}", " ", (s or "")).strip()

def pick_first_by_labels(text: str, labels: list[str], maxlen: int = 140) -> str:
    if not text:
        return "N/A"
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    for i, ln in enumerate(lines):
        for lb in labels:
            if lb in ln:
                after = ln.split(lb, 1)[1]
                after = re.sub(r"^[\s:\-Â·â€¢\)]+", "", after).strip()
                after = _norm_ws(after)

                if len(after) < 2 and i + 1 < len(lines):
                    nxt = _norm_ws(lines[i + 1])
                    if nxt and not any(x in nxt for x in labels):
                        after = _norm_ws((after + " " + nxt).strip())

                if after:
                    return after[:maxlen].strip()
    return "N/A"

def pick_multi_by_labels(text: str, labels: list[str], max_items: int = 6, maxlen_each: int = 90) -> str:
    if not text:
        return "N/A"
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    hits = []
    for i, ln in enumerate(lines):
        for lb in labels:
            if lb in ln:
                after = ln.split(lb, 1)[1]
                after = re.sub(r"^[\s:\-Â·â€¢\)]+", "", after).strip()
                after = _norm_ws(after)
                if len(after) < 2 and i + 1 < len(lines):
                    nxt = _norm_ws(lines[i + 1])
                    if nxt and lb not in nxt:
                        after = _norm_ws((after + " " + nxt).strip())
                if after:
                    hits.append(f"{lb}: {after[:maxlen_each].strip()}")

    uniq, seen = [], set()
    for h in hits:
        key = re.sub(r"\s+", " ", h)
        if key in seen:
            continue
        seen.add(key)
        uniq.append(h)
        if len(uniq) >= max_items:
            break

    if not uniq:
        return "N/A"
    out = " / ".join(uniq)
    return out[:420] + ("â€¦" if len(out) > 420 else "")

def extract_money_purpose(text: str) -> str:
    v = pick_first_by_labels(text, [
        "ìê¸ˆì¡°ë‹¬ì˜ ëª©ì ", "ìê¸ˆì¡°ë‹¬ ëª©ì ", "ìê¸ˆì¡°ë‹¬ì˜ëª©ì ",
        "ìê¸ˆì˜ ì‚¬ìš©ëª©ì ", "ìê¸ˆì‚¬ìš©ëª©ì ", "ìê¸ˆ ì‚¬ìš© ëª©ì ",
        "ì¡°ë‹¬ìê¸ˆì˜ ì‚¬ìš©ëª©ì ", "ì¡°ë‹¬ ìê¸ˆì˜ ì‚¬ìš©ëª©ì ",
    ], maxlen=220)
    if v != "N/A":
        return v

    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    keys = ["ì‹œì„¤", "ìš´ì˜", "ì±„ë¬´", "íƒ€ë²•ì¸", "ê¸°íƒ€", "ì—°êµ¬", "M&A", "ì¸ìˆ˜", "íˆ¬ì"]
    amt_pat = re.compile(r"(\d{1,3}(?:,\d{3})+|\d+)\s*ì›")
    hits = []
    for ln in lines:
        if any(k in ln for k in keys) and amt_pat.search(ln):
            hits.append(_norm_ws(ln))
        if len(hits) >= 4:
            break
    if hits:
        out = " / ".join(hits)
        return out[:240] + ("â€¦" if len(out) > 240 else "")
    return "N/A"

def extract_fields(doc_text: str) -> dict:
    fields = {}
    fields["ìê¸ˆì¡°ë‹¬ì˜ëª©ì "] = extract_money_purpose(doc_text)

    fields["ì‹ ì£¼ë°°ì •ê¸°ì¤€ì¼"] = pick_first_by_labels(doc_text, [
        "ì‹ ì£¼ë°°ì •ê¸°ì¤€ì¼", "ì‹ ì£¼ ë°°ì • ê¸°ì¤€ì¼", "ë°°ì •ê¸°ì¤€ì¼", "ë°°ì • ê¸°ì¤€ì¼",
        "ì‹ ì£¼ë°°ì • ê¸°ì¤€ì¼", "ê¶Œë¦¬ë½ ê¸°ì¤€ì¼",
    ])

    fields["ì˜ˆì •ê°€"] = pick_first_by_labels(doc_text, [
        "ì˜ˆì •ë°œí–‰ê°€ì•¡", "ì˜ˆì • ë°œí–‰ê°€ì•¡", "ë°œí–‰ê°€ì•¡(ì˜ˆì •)", "ë°œí–‰ê°€ì•¡ (ì˜ˆì •)",
        "ì˜ˆì •ë°œí–‰ê°€", "ì˜ˆì • ë°œí–‰ê°€", "ì˜ˆì •ê°€", "ì˜ˆì •ê°€ì•¡",
        "1ì£¼ë‹¹ ë°œí–‰ê°€ì•¡(ì˜ˆì •)", "1ì£¼ë‹¹ ë°œí–‰ê°€ì•¡ (ì˜ˆì •)",
    ], maxlen=160)

    fields["í™•ì •ì¼"] = pick_first_by_labels(doc_text, [
        "ë°œí–‰ê°€ì•¡í™•ì •ì¼", "ë°œí–‰ê°€ì•¡ í™•ì •ì¼",
        "í™•ì •ì¼", "ê°€ê²©í™•ì •ì¼", "ê°€ê²© í™•ì •ì¼",
        "ë°œí–‰ê°€ í™•ì •ì¼", "ë°œí–‰ê°€ì•¡ì˜ í™•ì •ì¼",
    ])

    fields["ì‹ ì£¼ì¸ìˆ˜ê¶Œìƒì¥ì˜ˆì •ê¸°ê°„"] = pick_first_by_labels(doc_text, [
        "ì‹ ì£¼ì¸ìˆ˜ê¶Œì¦ì„œ ìƒì¥ì˜ˆì •ê¸°ê°„", "ì‹ ì£¼ì¸ìˆ˜ê¶Œì¦ì„œìƒì¥ì˜ˆì •ê¸°ê°„",
        "ì‹ ì£¼ì¸ìˆ˜ê¶Œ ìƒì¥ì˜ˆì •ê¸°ê°„", "ì‹ ì£¼ì¸ìˆ˜ê¶Œìƒì¥ì˜ˆì •ê¸°ê°„",
        "ì‹ ì£¼ì¸ìˆ˜ê¶Œì¦ì„œ ìƒì¥ê¸°ê°„", "ì‹ ì£¼ì¸ìˆ˜ê¶Œì¦ì„œìƒì¥ê¸°ê°„",
        "ì‹ ì£¼ì¸ìˆ˜ê¶Œ ìƒì¥ê¸°ê°„", "ì‹ ì£¼ì¸ìˆ˜ê¶Œìƒì¥ê¸°ê°„",
    ], maxlen=200)

    fields["ì²­ì•½ì¼"] = pick_multi_by_labels(doc_text, [
        "ìš°ë¦¬ì‚¬ì£¼ì¡°í•© ì²­ì•½ì¼", "ìš°ë¦¬ì‚¬ì£¼ì¡°í•©ì²­ì•½ì¼",
        "êµ¬ì£¼ì£¼ ì²­ì•½ì¼", "êµ¬ì£¼ì£¼ì²­ì•½ì¼",
        "ì¼ë°˜ê³µëª¨ ì²­ì•½ì¼", "ì¼ë°˜ê³µëª¨ì²­ì•½ì¼",
        "ì¼ë°˜ì²­ì•½ì¼",
        "ì²­ì•½ì¼",
    ])

    fields["ì‹ ì£¼ì˜ìƒì¥ì˜ˆì •ì¼"] = pick_first_by_labels(doc_text, [
        "ì‹ ì£¼ì˜ ìƒì¥ì˜ˆì •ì¼", "ì‹ ì£¼ì˜ìƒì¥ì˜ˆì •ì¼",
        "ì‹ ì£¼ ìƒì¥ì˜ˆì •ì¼", "ì‹ ì£¼ìƒì¥ì˜ˆì •ì¼",
        "ì‹ ì£¼ê¶Œ ìƒì¥ì˜ˆì •ì¼", "ì‹ ì£¼ê¶Œìƒì¥ì˜ˆì •ì¼",
        "ìƒì¥ì˜ˆì •ì¼",
    ])

    return fields

# =========================
# N/A ìˆ¨ê¹€ + ì¹´ë“œ ë Œë”ë§ + ìœ„í—˜ë„
# =========================
def _is_empty_value(v: str) -> bool:
    if v is None:
        return True
    s = str(v).strip()
    if not s:
        return True
    if s.upper() == "N/A":
        return True
    if s in ("0", "0ì›", "0 ì£¼", "0ì£¼"):
        return True
    return False

def add_if(lines: list[str], label: str, value: str):
    if _is_empty_value(value):
        return
    lines.append(f"â€¢ <b>{html.escape(label)}</b>: {html.escape(value)}")

def risk_score(ev_type: str, alloc: str, market: str, doc_text: str, fields: dict) -> tuple[int, str, str]:
    """
    0~100 íœ´ë¦¬ìŠ¤í‹±.
    - ìœ ìƒ/ìœ ë¬´ìƒ > ë¬´ìƒ
    - KOSDAQ/KONEX ê°€ì¤‘
    - ì±„ë¬´/ìš´ì˜ ëª©ì  ê°€ì¤‘
    - ì²­ì•½/ì˜ˆì •ê°€/í™•ì •ì¼/ì¸ìˆ˜ê¶Œê¸°ê°„/ìƒì¥ì˜ˆì •ì¼ ì •ë³´ê°€ ë§ì„ìˆ˜ë¡(=ìœ ìƒ ì„±ê²©) ê°€ì¤‘
    """
    score = 10

    et = (ev_type or "").strip()
    if et == "ìœ ìƒ":
        score += 40
    elif et == "ìœ ë¬´ìƒ":
        score += 30
    elif et == "ë¬´ìƒ":
        score += 10
    else:
        score += 15

    mk = (market or "").upper()
    if mk == "KOSDAQ":
        score += 10
    elif mk == "KONEX":
        score += 15
    elif mk == "KOSPI":
        score += 5
    else:
        score += 7

    # ë°°ì • ë°©ì‹
    if alloc == "ì£¼ì£¼ë°°ì •":
        score += 8
    elif alloc == "ì¼ë°˜ì£¼ì£¼ë°°ì •":
        score += 6

    # ëª©ì  í‚¤ì›Œë“œ
    purpose = (fields.get("ìê¸ˆì¡°ë‹¬ì˜ëª©ì ") or "")
    if re.search(r"(ì±„ë¬´|ìƒí™˜|ì°¨ì…|ëŒ€ì¶œ)", purpose):
        score += 18
    if re.search(r"(ìš´ì˜|ìš´ì „ìê¸ˆ)", purpose):
        score += 10
    if re.search(r"(íƒ€ë²•ì¸|M&A|ì¸ìˆ˜|ì·¨ë“|íˆ¬ì)", purpose):
        score += 12

    # ì¼ì •/ê°€ê²© ì •ë³´ê°€ ë§ì´ ì¡íˆë©´ ì‹¤ì œ ì²­ì•½/ë°œí–‰ í”„ë¡œì„¸ìŠ¤ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
    for k in ["ì²­ì•½ì¼", "ì˜ˆì •ê°€", "í™•ì •ì¼", "ì‹ ì£¼ì¸ìˆ˜ê¶Œìƒì¥ì˜ˆì •ê¸°ê°„", "ì‹ ì£¼ì˜ìƒì¥ì˜ˆì •ì¼"]:
        if not _is_empty_value(fields.get(k, "N/A")):
            score += 4

    # ì›ë¬¸ì—ì„œ "í• ì¸" "ë³´í†µì£¼" ë“±ë„ ì•½ê°„ ë°˜ì˜(ê°€ë²¼ìš´ íŒíŠ¸)
    if re.search(r"(í• ì¸|ë°œí–‰ê°€ì•¡|ì¸ìˆ˜ê¶Œ)", doc_text):
        score += 4

    # clamp
    score = max(0, min(100, score))

    if score >= 75:
        emoji, grade = "ğŸ”´", "ë†’ìŒ"
    elif score >= 55:
        emoji, grade = "ğŸŸ ", "ì¤‘ê°„"
    elif score >= 35:
        emoji, grade = "ğŸŸ¡", "ë‚®ìŒ"
    else:
        emoji, grade = "ğŸŸ¢", "ë§¤ìš°ë‚®ìŒ"
    return score, grade, emoji

def build_card(corp: str, market: str, ev_type: str, alloc: str, rcept_dt: str, rpt_nm: str, url: str,
               doc_text: str, fields: dict) -> str:
    score, grade, emoji = risk_score(ev_type, alloc, market, doc_text, fields)

    # ì¹´ë“œ í—¤ë”
    lines = []
    lines.append(f"{emoji} <b>ì¦ì ê³µì‹œ ê°ì§€</b>  <i>(ìœ„í—˜ë„ {score}/100 Â· {grade})</i>")
    lines.append(f"ğŸ¢ <b>{html.escape(corp)}</b>  <i>({html.escape(market)})</i>")
    lines.append(f"ğŸ§¾ ìœ í˜•: <b>{html.escape(ev_type)}</b> / ë°°ì •: <b>{html.escape(alloc)}</b>")
    if rcept_dt:
        lines.append(f"ğŸ“… ì ‘ìˆ˜ì¼: {html.escape(rcept_dt)}")
    lines.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    lines.append(f"ğŸ“Œ <b>ê³µì‹œëª…</b>")
    lines.append(f"{html.escape(rpt_nm)}")
    lines.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # í•µì‹¬ ìš”ì•½(í•„ë“œ ì¤‘ ê°’ ìˆëŠ” ê²ƒë§Œ ë…¸ì¶œ)
    core = []
    add_if(core, "ìê¸ˆì¡°ë‹¬ì˜ëª©ì ", fields.get("ìê¸ˆì¡°ë‹¬ì˜ëª©ì ", "N/A"))
    add_if(core, "ì‹ ì£¼ë°°ì •ê¸°ì¤€ì¼", fields.get("ì‹ ì£¼ë°°ì •ê¸°ì¤€ì¼", "N/A"))
    if core:
        lines.append("ğŸ§  <b>í•µì‹¬</b>")
        lines.extend(core)
        lines.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # ê°€ê²©/ì¼ì • ì„¹ì…˜
    price = []
    add_if(price, "ì˜ˆì •ê°€", fields.get("ì˜ˆì •ê°€", "N/A"))
    add_if(price, "í™•ì •ì¼", fields.get("í™•ì •ì¼", "N/A"))
    if price:
        lines.append("ğŸ’° <b>ê°€ê²©</b>")
        lines.extend(price)
        lines.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    sched = []
    add_if(sched, "ì‹ ì£¼ì¸ìˆ˜ê¶Œìƒì¥ì˜ˆì •ê¸°ê°„", fields.get("ì‹ ì£¼ì¸ìˆ˜ê¶Œìƒì¥ì˜ˆì •ê¸°ê°„", "N/A"))
    add_if(sched, "ì²­ì•½ì¼", fields.get("ì²­ì•½ì¼", "N/A"))
    add_if(sched, "ì‹ ì£¼ì˜ìƒì¥ì˜ˆì •ì¼", fields.get("ì‹ ì£¼ì˜ìƒì¥ì˜ˆì •ì¼", "N/A"))
    if sched:
        lines.append("ğŸ—“ï¸ <b>ì¼ì •</b>")
        lines.extend(sched)
        lines.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # ë§¨ ì•„ë˜ ë§í¬ ë¬¸êµ¬(ë²„íŠ¼ì´ ìˆìœ¼ë‹ˆ í…ìŠ¤íŠ¸ëŠ” ì§§ê²Œ)
    lines.append("â¡ï¸ ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ì›ë¬¸ í™•ì¸")

    return "\n".join(lines)

# =========================
# main
# =========================
def main():
    st = load_state()
    sent = 0

    items = fetch_list_pages()

    for it in items:
        rno = (it.get("rcept_no") or "").strip()
        if not rno:
            continue
        if is_seen(st, rno):
            continue

        corp_cls = (it.get("corp_cls") or "").strip().upper()
        if not market_ok(corp_cls):
            mark_seen(st, rno)
            continue

        rpt_nm = (it.get("report_nm") or "").strip()
        if not INC_REPORT.search(rpt_nm):
            mark_seen(st, rno)
            continue

        # ì›ë¬¸ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        try:
            doc_text = fetch_document_text(rno)
        except Exception:
            mark_seen(st, rno)
            continue

        # ì œ3ìë°°ì • í¬í•¨ì´ë©´ ì œì™¸
        if EXC_3RD.search(doc_text):
            mark_seen(st, rno)
            continue

        alloc = classify_allocation(doc_text)
        if alloc == "N/A":
            mark_seen(st, rno)
            continue

        fields = extract_fields(doc_text)
        ev_type = classify_event_type(rpt_nm)

        corp = (it.get("corp_name") or "N/A").strip()
        rcept_dt = (it.get("rcept_dt") or "").strip()
        url = VIEW_URL.format(rno)
        market_name = {"Y": "KOSPI", "K": "KOSDAQ", "N": "KONEX", "E": "OTHER"}.get(corp_cls, corp_cls or "N/A")

        msg = build_card(
            corp=corp,
            market=market_name,
            ev_type=ev_type,
            alloc=alloc,
            rcept_dt=rcept_dt,
            rpt_nm=rpt_nm,
            url=url,
            doc_text=doc_text,
            fields=fields
        )

        tg_send_safe(msg, button_url=url)

        mark_seen(st, rno)
        sent += 1

    save_state(st)
    print(f"OK sent={sent} seen={len(st.get('seen', []))}")

if __name__ == "__main__":
    main()
