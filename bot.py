#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, json, re, io, zipfile, html
import requests
from datetime import datetime, timedelta

# =========================
# Secrets (GitHub repo Secrets)
# =========================
DART_API_KEY = os.environ["DART_API_KEY"].strip()
TG_BOT_TOKEN = os.environ["TG_BOT_TOKEN"].strip()
TG_CHAT_ID   = os.environ["TG_CHAT_ID"].strip()

# =========================
# Config (workflow env override ê°€ëŠ¥)
# =========================
LOOKBACK_DAYS = int(os.getenv("LOOKBACK_DAYS", "3"))

# ì‹œì¥ í•„í„° (ë¹ˆ ê°’ì´ë©´ ì „ì²´)
# Y=KOSPI, K=KOSDAQ, N=KONEX, E=OTHER
MARKET_CLASSES = [x.strip().upper() for x in os.getenv("MARKET_CLASSES", "Y,K,N").split(",") if x.strip()]

# í˜ì´ì§€ë„¤ì´ì…˜
MAX_PAGES  = int(os.getenv("MAX_PAGES", "12"))
PAGE_COUNT = int(os.getenv("PAGE_COUNT", "100"))

STATE_PATH = "state.json"
SEEN_MAX   = 8000

# =========================
# Filters (ì •ì±…)
# =========================
# 1) ë³´ê³ ì„œëª…: ìœ ìƒ/ë¬´ìƒ/ìœ ë¬´ìƒ "ê²°ì •"ë§Œ ëŒ€ìƒìœ¼ë¡œ
INC_REPORT = re.compile(
    r"(ìœ ìƒì¦ì\s*ê²°ì •|ìœ ìƒì¦ìê²°ì •|ë¬´ìƒì¦ì\s*ê²°ì •|ë¬´ìƒì¦ìê²°ì •|ìœ ë¬´ìƒì¦ì\s*ê²°ì •|ìœ ë¬´ìƒì¦ìê²°ì •)",
    re.I
)

# 2) ì›ë¬¸ì—ì„œ ì œ3ìë°°ì •ì€ ë¬´ì¡°ê±´ ì œì™¸
EXC_3RD = re.compile(r"(ì œ\s*3\s*ì\s*ë°°ì •|ì œ3ìë°°ì •)", re.I)

# 3) ì›ë¬¸ì—ì„œ í—ˆìš©ë˜ëŠ” ë°°ì • ë°©ì‹
# - ì¼ë°˜ì£¼ì£¼ë°°ì •
ALLOW_GENERAL = re.compile(r"(ì¼ë°˜\s*ì£¼ì£¼\s*ë°°ì •|ì¼ë°˜ì£¼ì£¼ë°°ì •)", re.I)
# - ì£¼ì£¼ë°°ì •(êµ¬ì£¼ì£¼ í¬í•¨)
ALLOW_SHAREHOLDER = re.compile(r"(ì£¼ì£¼\s*ë°°ì •|ì£¼ì£¼ë°°ì •|êµ¬ì£¼ì£¼\s*ì²­ì•½|êµ¬ì£¼ì£¼ì²­ì•½|êµ¬ì£¼ì£¼)", re.I)

# =========================
# URLs
# =========================
LIST_URL = "https://opendart.fss.or.kr/api/list.json"
DOC_URL  = "https://opendart.fss.or.kr/api/document.xml"
VIEW_URL = "https://dart.fss.or.kr/dsaf001/main.do?rcpNo={}"
TG_SEND  = "https://api.telegram.org/bot{}/sendMessage"

S = requests.Session()
S.headers.update({"User-Agent": "dart-alert-github-actions/2.1"})

TG_MAX = 4096

# =========================
# state.json
# =========================
def load_state():
    try:
        with open(STATE_PATH, "r", encoding="utf-8") as f:
            st = json.load(f)
        if not isinstance(st, dict):
            return {"seen": []}
        if "seen" not in st or not isinstance(st["seen"], list):
            st["seen"] = []
        return st
    except Exception:
        return {"seen": []}

def save_state(st):
    seen = st.get("seen", [])
    if not isinstance(seen, list):
        seen = []
    st["seen"] = seen[-SEEN_MAX:]
    with open(STATE_PATH, "w", encoding="utf-8") as f:
        json.dump(st, f, ensure_ascii=False, indent=2)

def is_seen(st, rcept_no: str) -> bool:
    return rcept_no in set(st.get("seen", []))

def mark_seen(st, rcept_no: str):
    st.setdefault("seen", []).append(rcept_no)

# =========================
# Telegram
# =========================
def tg_send(text: str, button_url: str | None = None):
    payload = {
        "chat_id": TG_CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": True,
    }
    if button_url:
        payload["reply_markup"] = json.dumps(
            {"inline_keyboard": [[{"text": "ğŸ“„ DART ì—´ê¸°", "url": button_url}]]},
            ensure_ascii=False
        )

    r = S.post(TG_SEND.format(TG_BOT_TOKEN), data=payload, timeout=30)
    r.raise_for_status()

def tg_send_safe(text: str, button_url: str | None = None):
    if len(text) <= TG_MAX:
        tg_send(text, button_url)
        return
    tg_send(text[: TG_MAX - 40] + "\n\n(â€¦ì¤‘ëµ)", button_url)

# =========================
# DART list.json (pagination)
# =========================
def market_ok(corp_cls: str) -> bool:
    corp_cls = (corp_cls or "").strip().upper()
    if not MARKET_CLASSES:
        return True
    return corp_cls in MARKET_CLASSES

def fetch_list_pages():
    end_de = datetime.now().strftime("%Y%m%d")
    bgn_de = (datetime.now() - timedelta(days=LOOKBACK_DAYS)).strftime("%Y%m%d")

    out = []
    for p in range(1, MAX_PAGES + 1):
        j = S.get(
            LIST_URL,
            params=dict(
                crtfc_key=DART_API_KEY,
                bgn_de=bgn_de,
                end_de=end_de,
                pblntf_ty="B",
                page_no=p,
                page_count=PAGE_COUNT,
                sort="date",
                sort_mth="desc",
            ),
            timeout=30,
        ).json()

        status = j.get("status")
        if status == "013":
            break
        if status != "000":
            raise RuntimeError(f"LIST error {status}: {j.get('message')}")

        lst = j.get("list") or []
        if not lst:
            break
        out.extend(lst)

        # total_count ê¸°ë°˜ ì¢…ë£Œ (ìˆì„ ë•Œë§Œ)
        try:
            total = int(j.get("total_count") or 0)
            pc    = int(j.get("page_count") or PAGE_COUNT)
            if total and total <= p * pc:
                break
        except Exception:
            pass

    # rcept_no ì¤‘ë³µ ì œê±°
    seen = set()
    dedup = []
    for it in out:
        rno = (it.get("rcept_no") or "").strip()
        if not rno or rno in seen:
            continue
        seen.add(rno)
        dedup.append(it)

    # ì˜¤ë˜ëœ ê²ƒë¶€í„° ì²˜ë¦¬
    dedup.sort(key=lambda x: (x.get("rcept_dt", ""), x.get("rcept_no", "")))
    return dedup

# =========================
# document.xml fetch + textify
# =========================
def _xml_to_text(xml_bytes: bytes) -> str:
    s = xml_bytes.decode("utf-8", errors="ignore")

    # ì¤„ë°”ê¿ˆ íŒíŠ¸
    s = re.sub(r"(?i)<br\s*/?>", "\n", s)
    s = re.sub(r"(?i)</(tr|p|div|li|h\d)>", "\n", s)

    # íƒœê·¸ ì œê±°
    s = re.sub(r"<[^>]+>", " ", s)

    # ì—”í‹°í‹° decode
    s = html.unescape(s)

    # ê³µë°± ì •ë¦¬
    s = re.sub(r"[ \t\r\f\v]+", " ", s)
    s = re.sub(r"\n\s+", "\n", s)
    s = re.sub(r"\n{3,}", "\n\n", s)

    return s.strip()

def fetch_document_text(rcept_no: str) -> str:
    r = S.get(DOC_URL, params={"crtfc_key": DART_API_KEY, "rcept_no": rcept_no}, timeout=60)
    r.raise_for_status()
    raw = r.content

    texts = []
    # ZIP íŒë³„
    is_zip = (r.headers.get("Content-Type", "").lower().find("zip") >= 0) or (raw[:2] == b"PK")
    if is_zip:
        with zipfile.ZipFile(io.BytesIO(raw)) as z:
            for name in z.namelist():
                if name.lower().endswith((".xml", ".html", ".htm")):
                    try:
                        texts.append(_xml_to_text(z.read(name)))
                    except Exception:
                        pass
    else:
        texts.append(_xml_to_text(raw))

    return "\n\n".join([t for t in texts if t])

# =========================
# classify allocation + type
# =========================
def classify_event_type(report_nm: str) -> str:
    rn = report_nm or ""
    if re.search(r"ë¬´ìƒì¦ì", rn):
        return "ë¬´ìƒ"
    if re.search(r"ìœ ë¬´ìƒì¦ì", rn):
        return "ìœ ë¬´ìƒ"
    if re.search(r"ìœ ìƒì¦ì", rn):
        return "ìœ ìƒ"
    return "N/A"

def classify_allocation(doc_text: str) -> str:
    """
    return: 'ì¼ë°˜ì£¼ì£¼ë°°ì •' | 'ì£¼ì£¼ë°°ì •' | 'N/A'
    """
    if ALLOW_GENERAL.search(doc_text):
        return "ì¼ë°˜ì£¼ì£¼ë°°ì •"
    if ALLOW_SHAREHOLDER.search(doc_text):
        return "ì£¼ì£¼ë°°ì •"
    return "N/A"

# =========================
# ì²­ì•½ì¼ì • ì¶”ì¶œ (ì›ë¬¸ ê¸°ë°˜)
# =========================
def _normalize_ws(s: str) -> str:
    s = re.sub(r"\s{2,}", " ", s).strip()
    return s

def extract_subscription_schedule(doc_text: str) -> str:
    """
    ì›ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ì²­ì•½ ê´€ë ¨ ë¼ì¸ì„ ì—¬ëŸ¬ ê°œ ìˆ˜ì§‘í•´ì„œ í•œ ì¤„ë¡œ í•©ì¹¨.
    - ìš°ë¦¬ì‚¬ì£¼/êµ¬ì£¼ì£¼/ì¼ë°˜ê³µëª¨ ë“± ë¼ë²¨ì´ ìˆìœ¼ë©´ ê·¸ ë¼ë²¨ ìš°ì„ 
    - ì—†ìœ¼ë©´ 'ì²­ì•½ì¼' ë¼ì¸ì´ë¼ë„ ìˆ˜ì§‘
    """
    if not doc_text:
        return "N/A"

    # ë¼ë²¨ í›„ë³´ (ìš°ì„ ìˆœìœ„)
    labels = [
        "ìš°ë¦¬ì‚¬ì£¼ì¡°í•© ì²­ì•½ì¼", "ìš°ë¦¬ì‚¬ì£¼ì¡°í•©ì²­ì•½ì¼",
        "êµ¬ì£¼ì£¼ ì²­ì•½ì¼", "êµ¬ì£¼ì£¼ì²­ì•½ì¼",
        "ì¼ë°˜ê³µëª¨ ì²­ì•½ì¼", "ì¼ë°˜ê³µëª¨ì²­ì•½ì¼",
        "ì¼ë°˜ì²­ì•½ì¼", "ì²­ì•½ì¼",
    ]

    lines = [ln.strip() for ln in doc_text.splitlines() if ln.strip()]
    hits = []

    # 1) ë¼ë²¨ ê¸°ë°˜ ìˆ˜ì§‘
    for ln in lines:
        for lb in labels:
            if lb in ln:
                after = ln.split(lb, 1)[1]
                after = re.sub(r"^[\s:\-Â·â€¢\)]+", "", after).strip()
                after = _normalize_ws(after)

                # ê°’ì´ ë¹„ë©´ ë‹¤ìŒ ë¼ì¸ ë¶™ì´ê¸°(í‘œ ë¶„ë¦¬ ëŒ€ë¹„)
                if len(after) < 2:
                    # ë‹¤ìŒ ì¤„ 1ê°œë§Œ ë¶™ì´ê¸°(ê³¼ë„ ê²°í•© ë°©ì§€)
                    idx = None
                    try:
                        idx = lines.index(ln)
                    except Exception:
                        idx = None
                    if idx is not None and idx + 1 < len(lines):
                        nxt = _normalize_ws(lines[idx + 1])
                        if nxt and lb not in nxt:
                            after = (after + " " + nxt).strip()

                if after:
                    hits.append(f"{lb}: {after}")

    # 2) ë¼ë²¨ì´ ì „í˜€ ì—†ìœ¼ë©´, 'ì²­ì•½' í‚¤ì›Œë“œ ë¼ì¸ ì¤‘ ë‚ ì§œ íŒ¨í„´ì´ ìˆëŠ” ê²ƒ ìˆ˜ì§‘
    if not hits:
        date_pat = re.compile(
            r"(\d{4}\s*[.\-/ë…„]\s*\d{1,2}\s*[.\-/ì›”]\s*\d{1,2}\s*ì¼?)"
            r"(\s*[~\-]\s*)?"
            r"(\d{4}\s*[.\-/ë…„]\s*\d{1,2}\s*[.\-/ì›”]\s*\d{1,2}\s*ì¼?)?",
            re.I
        )
        for ln in lines:
            if "ì²­ì•½" in ln:
                m = date_pat.search(ln)
                if m:
                    hits.append(_normalize_ws(ln))

    # ì¤‘ë³µ ì œê±°(ìˆœì„œ ìœ ì§€)
    uniq = []
    seen = set()
    for h in hits:
        key = re.sub(r"\s+", " ", h)
        if key in seen:
            continue
        seen.add(key)
        uniq.append(h)

    if not uniq:
        return "N/A"

    # ë„ˆë¬´ ê¸¸ë©´ ì ë‹¹íˆ ìë¥´ê¸°
    out = " / ".join(uniq)
    if len(out) > 350:
        out = out[:350] + "â€¦"
    return out

# =========================
# main
# =========================
def main():
    st = load_state()
    sent = 0

    items = fetch_list_pages()

    for it in items:
        rno = (it.get("rcept_no") or "").strip()
        if not rno:
            continue
        if is_seen(st, rno):
            continue

        corp_cls = (it.get("corp_cls") or "").strip().upper()
        if not market_ok(corp_cls):
            mark_seen(st, rno)
            continue

        rpt_nm = (it.get("report_nm") or "").strip()
        if not INC_REPORT.search(rpt_nm):
            mark_seen(st, rno)
            continue

        # ì›ë¬¸ í…ìŠ¤íŠ¸ ê°€ì ¸ì™€ì„œ ë°°ì •ë°©ì‹/ì œ3ì ì—¬ë¶€ íŒì •
        try:
            doc_text = fetch_document_text(rno)
        except Exception:
            # ì›ë¬¸ì„ ëª» ê°€ì ¸ì˜¤ë©´ ì •ì±…ìƒ ì•ˆì „í•˜ê²Œ ì œì™¸(ì›ë¬¸ ê¸°ë°˜ í•„í„°ë¼ì„œ)
            mark_seen(st, rno)
            continue

        # ì œ3ìë°°ì • í¬í•¨ì´ë©´ ë¬´ì¡°ê±´ ì œì™¸
        if EXC_3RD.search(doc_text):
            mark_seen(st, rno)
            continue

        alloc = classify_allocation(doc_text)
        if alloc == "N/A":
            # ì¼ë°˜ì£¼ì£¼/ì£¼ì£¼ë°°ì •ì´ ì•„ë‹Œ ì¼€ì´ìŠ¤ëŠ” ì œì™¸
            mark_seen(st, rno)
            continue

        # âœ… ì²­ì•½ì¼ì • ì¶”ì¶œ
        subs = extract_subscription_schedule(doc_text)

        ev_type = classify_event_type(rpt_nm)

        corp = (it.get("corp_name") or "N/A").strip()
        rcept_dt = (it.get("rcept_dt") or "").strip()
        url = VIEW_URL.format(rno)
        market_name = {"Y": "KOSPI", "K": "KOSDAQ", "N": "KONEX", "E": "OTHER"}.get(corp_cls, corp_cls or "N/A")

        lines = [
            "ğŸ“Œ <b>ì¦ì ê³µì‹œ ê°ì§€</b>",
            f"â€¢ íšŒì‚¬: <b>{html.escape(corp)}</b> <i>({html.escape(market_name)})</i>",
            f"â€¢ ìœ í˜•: <b>{html.escape(ev_type)}</b> / ë°°ì •: <b>{html.escape(alloc)}</b>",
            f"â€¢ ì ‘ìˆ˜ì¼: {html.escape(rcept_dt or 'N/A')}",
            f"â€¢ ê³µì‹œëª…: {html.escape(rpt_nm)}",
            f"â€¢ ì²­ì•½ì¼ì •: {html.escape(subs)}",
        ]

        tg_send_safe("\n".join(lines), button_url=url)
        mark_seen(st, rno)
        sent += 1

    save_state(st)
    print(f"OK sent={sent} seen={len(st.get('seen', []))}")

if __name__ == "__main__":
    main()
